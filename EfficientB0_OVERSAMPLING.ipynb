{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a31366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from tensorflow.keras.mixed_precision import set_global_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295c73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (32, 32)\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "EPOCHS = 100\n",
    "BASE_LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "633035e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)  # Resize to match input size of the model\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image, label\n",
    "\n",
    "def create_dataset(image_paths, labels, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: load_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n",
    "    return dataset\n",
    "\n",
    "def extract_paths_and_labels(directory_path, label):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subdir, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                image_paths.append(os.path.join(subdir, file))\n",
    "                labels.append(label)\n",
    "    return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e636e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dir = './data/processed/real/'\n",
    "fake_dir = './data/processed/fake/'\n",
    "\n",
    "# Extract paths and labels\n",
    "real_paths, real_labels = extract_paths_and_labels(real_dir, label=0)\n",
    "fake_paths, fake_labels = extract_paths_and_labels(fake_dir, label=1)\n",
    "\n",
    "# Combine paths and labels\n",
    "all_paths = np.array(real_paths + fake_paths)\n",
    "all_labels = np.array(real_labels + fake_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf4c26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80353"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7ddb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80353"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e21ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to directories\n",
    "\n",
    "# # Apply undersampling to balance classes\n",
    "# rus = RandomOverSampler(random_state=42)\n",
    "# all_paths_resampled, all_labels_resampled = rus.fit_resample(all_paths.reshape(-1, 1), all_labels)\n",
    "# all_paths_resampled = all_paths_resampled.flatten()\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_paths, \n",
    "    all_labels, \n",
    "    test_size=0.4, \n",
    "    random_state=42, \n",
    "    stratify=all_labels\n",
    ")\n",
    "\n",
    "rus = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train.reshape(-1, 1), y_train)\n",
    "X_train_resampled = X_train_resampled.flatten()\n",
    "\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = create_dataset(X_train_resampled, y_train_resampled, shuffle=True)\n",
    "test_dataset = create_dataset(X_test, y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4632de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label distribution: Counter({1: 43636, 0: 43636})\n",
      "Test label distribution: Counter({1: 29092, 0: 3050})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Check the distribution of training labels\n",
    "train_label_counts = Counter(y_train_resampled)\n",
    "print(\"Training label distribution:\", train_label_counts)\n",
    "\n",
    "# Check the distribution of test labels\n",
    "test_label_counts = Counter(y_test)\n",
    "print(\"Test label distribution:\", test_label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97fc394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 1, 1, 1280)       4049571   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,412,068\n",
      "Trainable params: 4,369,277\n",
      "Non-trainable params: 42,791\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the improved model\n",
    "\n",
    "def build_deepfake_model(input_shape=INPUT_SHAPE):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Add fully connected layers with dropout and batch normalization\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=BASE_LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_deepfake_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575806bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1650, compute capability 7.5\n",
      "GPU setup error: Physical devices cannot be modified after being initialized\n",
      "Epoch 1/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 1.2197 - accuracy: 0.6560\n",
      "Epoch 1: val_loss improved from inf to 0.74321, saving model to Models\\Eb0_OVS_best_model_weights.h5\n",
      "2728/2728 [==============================] - 359s 114ms/step - loss: 1.2197 - accuracy: 0.6560 - val_loss: 0.7432 - val_accuracy: 0.5658 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5036 - accuracy: 0.7742\n",
      "Epoch 2: val_loss did not improve from 0.74321\n",
      "2728/2728 [==============================] - 297s 109ms/step - loss: 0.5036 - accuracy: 0.7742 - val_loss: 1.0461 - val_accuracy: 0.6820 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.3955 - accuracy: 0.8374\n",
      "Epoch 3: val_loss did not improve from 0.74321\n",
      "2728/2728 [==============================] - 257s 94ms/step - loss: 0.3955 - accuracy: 0.8374 - val_loss: 0.9998 - val_accuracy: 0.5436 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5015 - accuracy: 0.7919\n",
      "Epoch 4: val_loss did not improve from 0.74321\n",
      "2728/2728 [==============================] - 244s 90ms/step - loss: 0.5015 - accuracy: 0.7919 - val_loss: 0.8285 - val_accuracy: 0.5064 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5159 - accuracy: 0.7794\n",
      "Epoch 5: val_loss improved from 0.74321 to 0.65464, saving model to Models\\Eb0_OVS_best_model_weights.h5\n",
      "2728/2728 [==============================] - 245s 90ms/step - loss: 0.5159 - accuracy: 0.7794 - val_loss: 0.6546 - val_accuracy: 0.7109 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.7695\n",
      "Epoch 6: val_loss improved from 0.65464 to 0.60386, saving model to Models\\Eb0_OVS_best_model_weights.h5\n",
      "2728/2728 [==============================] - 245s 90ms/step - loss: 0.5267 - accuracy: 0.7695 - val_loss: 0.6039 - val_accuracy: 0.7447 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5144 - accuracy: 0.7775\n",
      "Epoch 7: val_loss did not improve from 0.60386\n",
      "2728/2728 [==============================] - 245s 90ms/step - loss: 0.5144 - accuracy: 0.7775 - val_loss: 0.7622 - val_accuracy: 0.4373 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.7103\n",
      "Epoch 8: val_loss did not improve from 0.60386\n",
      "2728/2728 [==============================] - 244s 90ms/step - loss: 0.5999 - accuracy: 0.7103 - val_loss: 0.7524 - val_accuracy: 0.3382 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5955 - accuracy: 0.7138\n",
      "Epoch 9: val_loss did not improve from 0.60386\n",
      "2728/2728 [==============================] - 245s 90ms/step - loss: 0.5955 - accuracy: 0.7138 - val_loss: 1.1776 - val_accuracy: 0.2289 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5554 - accuracy: 0.7426\n",
      "Epoch 10: val_loss did not improve from 0.60386\n",
      "2728/2728 [==============================] - 245s 90ms/step - loss: 0.5554 - accuracy: 0.7426 - val_loss: 0.7555 - val_accuracy: 0.4615 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.6061 - accuracy: 0.7032\n",
      "Epoch 11: val_loss did not improve from 0.60386\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2728/2728 [==============================] - 244s 90ms/step - loss: 0.6061 - accuracy: 0.7032 - val_loss: 0.6313 - val_accuracy: 0.6826 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5730 - accuracy: 0.7051\n",
      "Epoch 12: val_loss did not improve from 0.60386\n",
      "2728/2728 [==============================] - 245s 90ms/step - loss: 0.5730 - accuracy: 0.7051 - val_loss: 0.7562 - val_accuracy: 0.5140 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5483 - accuracy: 0.7216\n",
      "Epoch 13: val_loss did not improve from 0.60386\n",
      "2728/2728 [==============================] - 244s 90ms/step - loss: 0.5483 - accuracy: 0.7216 - val_loss: 0.7307 - val_accuracy: 0.5471 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5367 - accuracy: 0.7303\n",
      "Epoch 14: val_loss improved from 0.60386 to 0.58926, saving model to Models\\Eb0_OVS_best_model_weights.h5\n",
      "2728/2728 [==============================] - 246s 90ms/step - loss: 0.5367 - accuracy: 0.7303 - val_loss: 0.5893 - val_accuracy: 0.7128 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.7196\n",
      "Epoch 15: val_loss did not improve from 0.58926\n",
      "2728/2728 [==============================] - 245s 90ms/step - loss: 0.5540 - accuracy: 0.7196 - val_loss: 0.7526 - val_accuracy: 0.5334 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.7234\n",
      "Epoch 16: val_loss did not improve from 0.58926\n",
      "2728/2728 [==============================] - 245s 90ms/step - loss: 0.5461 - accuracy: 0.7234 - val_loss: 0.6837 - val_accuracy: 0.6219 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5290 - accuracy: 0.7350\n",
      "Epoch 17: val_loss did not improve from 0.58926\n",
      "2728/2728 [==============================] - 244s 90ms/step - loss: 0.5290 - accuracy: 0.7350 - val_loss: 0.6786 - val_accuracy: 0.6110 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5265 - accuracy: 0.7353\n",
      "Epoch 18: val_loss did not improve from 0.58926\n",
      "2728/2728 [==============================] - 251s 92ms/step - loss: 0.5265 - accuracy: 0.7353 - val_loss: 0.7496 - val_accuracy: 0.5628 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.7381\n",
      "Epoch 19: val_loss did not improve from 0.58926\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "2728/2728 [==============================] - 248s 91ms/step - loss: 0.5270 - accuracy: 0.7381 - val_loss: 0.6530 - val_accuracy: 0.6650 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5330 - accuracy: 0.7320\n",
      "Epoch 20: val_loss did not improve from 0.58926\n",
      "2728/2728 [==============================] - 249s 91ms/step - loss: 0.5330 - accuracy: 0.7320 - val_loss: 0.7719 - val_accuracy: 0.5663 - lr: 4.0000e-05\n",
      "Epoch 21/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.7324\n",
      "Epoch 21: val_loss did not improve from 0.58926\n",
      "2728/2728 [==============================] - 249s 91ms/step - loss: 0.5269 - accuracy: 0.7324 - val_loss: 0.7503 - val_accuracy: 0.5804 - lr: 4.0000e-05\n",
      "Epoch 22/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5214 - accuracy: 0.7365\n",
      "Epoch 22: val_loss did not improve from 0.58926\n",
      "2728/2728 [==============================] - 249s 91ms/step - loss: 0.5214 - accuracy: 0.7365 - val_loss: 0.7610 - val_accuracy: 0.5690 - lr: 4.0000e-05\n",
      "Epoch 23/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.7394\n",
      "Epoch 23: val_loss did not improve from 0.58926\n",
      "2728/2728 [==============================] - 250s 91ms/step - loss: 0.5165 - accuracy: 0.7394 - val_loss: 0.7397 - val_accuracy: 0.5814 - lr: 4.0000e-05\n",
      "Epoch 24/100\n",
      "2728/2728 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.7429\n",
      "Epoch 24: val_loss did not improve from 0.58926\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "2728/2728 [==============================] - 249s 91ms/step - loss: 0.5114 - accuracy: 0.7429 - val_loss: 0.7517 - val_accuracy: 0.5895 - lr: 4.0000e-05\n",
      "Epoch 24: early stopping\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 51\u001b[0m\n\u001b[0;32m     42\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     43\u001b[0m     train_dataset,\n\u001b[0;32m     44\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m1.0\u001b[39m}  \u001b[38;5;66;03m# Balance weights if needed\u001b[39;00m\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Save the entire model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mModels/Eb0_OVS_best_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "\n",
    "# Enable mixed precision training\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU setup error: {e}\")\n",
    "    \n",
    "    \n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='Models/Eb0_OVS_best_model_weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Cosine decay learning rate scheduler\n",
    "cosine_decay = CosineDecay(initial_learning_rate=BASE_LEARNING_RATE, decay_steps=EPOCHS, alpha=0.1)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback],\n",
    "    class_weight={0: 1.0, 1: 1.0}  # Balance weights if needed\n",
    ")\n",
    "\n",
    "# Save the entire model\n",
    "model.save('Models/Eb0_OVS_best_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4231478c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005/1005 [==============================] - 19s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "y_pred_probs = model.predict(test_dataset, verbose=1)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "030d728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.71      0.32      3050\n",
      "           1       0.96      0.71      0.82     29092\n",
      "\n",
      "    accuracy                           0.71     32142\n",
      "   macro avg       0.58      0.71      0.57     32142\n",
      "weighted avg       0.89      0.71      0.77     32142\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2156   894]\n",
      " [ 8361 20731]]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the true labels and predictions\n",
    "y_test_flat = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "y_pred_flat = y_pred.flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_flat, y_pred_flat))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_flat, y_pred_flat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a065e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test_flat, y_pred_flat)\n",
    "plt.figure(figsize=(8, 6))  # You can adjust the size of the figure\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fe11bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Step 1: Extract frames from the video\n",
    "def extract_frames_from_video(video_path, fps=5):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_interval = max(1, int(video_fps / fps))\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frames.append(frame)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Step 2: Preprocess the frames\n",
    "def preprocess_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = tf.convert_to_tensor(frame, dtype=tf.float32)\n",
    "    frame = tf.image.resize(frame, [32, 32])\n",
    "    frame = frame / 255.0\n",
    "    return tf.expand_dims(frame, axis=0)\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    return np.vstack([preprocess_frame(frame) for frame in frames])\n",
    "\n",
    "\n",
    "def predict_on_video(video_path):\n",
    "    frames = extract_frames_from_video(video_path, fps=1)\n",
    "    preprocessed_frames = preprocess_frames(frames)\n",
    "    predictions = model_architecture.predict(preprocessed_frames)\n",
    "    predicted_labels = (predictions > 0.5).astype(int)\n",
    "    return predicted_labels\n",
    "\n",
    "# Step 4: Aggregate the predictions\n",
    "def aggregate_predictions(predicted_labels):\n",
    "    real_count = np.sum(predicted_labels == 0)\n",
    "    fake_count = np.sum(predicted_labels == 1)\n",
    "    print(real_count ,\"\",fake_count)\n",
    "    if fake_count > real_count:\n",
    "        return 'Fake'\n",
    "    else:\n",
    "        return 'Real'\n",
    "\n",
    "# Example: Predict for a video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18911334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Make predictions on the frames\n",
    "model_architecture = build_deepfake_model()  # Replace with your model creation function\n",
    "# Load the saved weights\n",
    "model_architecture.load_weights('Models/Eb0_OVS_best_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bac01384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 15s 15s/step\n",
      "6  8\n",
      "The video is classified as: Fake\n"
     ]
    }
   ],
   "source": [
    "video_path = './data/YouTube-real/00001.mp4'\n",
    "predicted_labels = predict_on_video(video_path)\n",
    "final_prediction = aggregate_predictions(predicted_labels)\n",
    "print(f\"The video is classified as: {final_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c583561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_videos_in_directory(directory_path):\n",
    "    results = []\n",
    "    \n",
    "    for video_file in os.listdir(directory_path):\n",
    "        video_path = os.path.join(directory_path, video_file)\n",
    "        \n",
    "        if video_file.endswith(('.mp4', '.avi', '.mov')):  # Check for video file formats\n",
    "            print(f\"Processing video: {video_file}\")\n",
    "            predicted_labels = predict_on_video(video_path)\n",
    "            final_prediction = aggregate_predictions(predicted_labels)\n",
    "            results.append((video_file, final_prediction))\n",
    "            print(f\"Prediction for {video_file}: {final_prediction}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e03c369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: id0_id16_0000.mp4\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "4  12\n",
      "Prediction for id0_id16_0000.mp4: Fake\n",
      "Processing video: id0_id16_0001.mp4\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "5  6\n",
      "Prediction for id0_id16_0001.mp4: Fake\n",
      "Processing video: id0_id16_0002.mp4\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "6  6\n",
      "Prediction for id0_id16_0002.mp4: Real\n",
      "Processing video: id0_id16_0003.mp4\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "7  11\n",
      "Prediction for id0_id16_0003.mp4: Fake\n",
      "Processing video: id0_id16_0004.mp4\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "2  9\n",
      "Prediction for id0_id16_0004.mp4: Fake\n",
      "Processing video: id0_id16_0005.mp4\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "6  10\n",
      "Prediction for id0_id16_0005.mp4: Fake\n",
      "Processing video: id0_id16_0006.mp4\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "7  11\n",
      "Prediction for id0_id16_0006.mp4: Fake\n",
      "Processing video: id0_id16_0007.mp4\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "8  8\n",
      "Prediction for id0_id16_0007.mp4: Real\n",
      "Processing video: id0_id16_0008.mp4\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "7  9\n",
      "Prediction for id0_id16_0008.mp4: Fake\n",
      "Processing video: id0_id16_0009.mp4\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "6  12\n",
      "Prediction for id0_id16_0009.mp4: Fake\n",
      "Processing video: id0_id17_0000.mp4\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "4  12\n",
      "Prediction for id0_id17_0000.mp4: Fake\n",
      "Processing video: id0_id17_0001.mp4\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "6  5\n",
      "Prediction for id0_id17_0001.mp4: Real\n",
      "Processing video: id0_id17_0002.mp4\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "9  3\n",
      "Prediction for id0_id17_0002.mp4: Real\n",
      "Processing video: id0_id17_0003.mp4\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "9  9\n",
      "Prediction for id0_id17_0003.mp4: Real\n",
      "Processing video: id0_id17_0005.mp4\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "4  12\n",
      "Prediction for id0_id17_0005.mp4: Fake\n",
      "Processing video: id0_id17_0006.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/Celeb-synthesis/\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to your directory containing videos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m video_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_on_videos_in_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display results for all videos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_file, prediction \u001b[38;5;129;01min\u001b[39;00m video_predictions:\n",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m, in \u001b[0;36mpredict_on_videos_in_directory\u001b[1;34m(directory_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m video_file\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.avi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mov\u001b[39m\u001b[38;5;124m'\u001b[39m)):  \u001b[38;5;66;03m# Check for video file formats\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     predicted_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_on_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     final_prediction \u001b[38;5;241m=\u001b[39m aggregate_predictions(predicted_labels)\n\u001b[0;32m     11\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((video_file, final_prediction))\n",
      "Cell \u001b[1;32mIn[21], line 38\u001b[0m, in \u001b[0;36mpredict_on_video\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_on_video\u001b[39m(video_path):\n\u001b[1;32m---> 38\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[43mextract_frames_from_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     preprocessed_frames \u001b[38;5;241m=\u001b[39m preprocess_frames(frames)\n\u001b[0;32m     40\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model_architecture\u001b[38;5;241m.\u001b[39mpredict(preprocessed_frames)\n",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m, in \u001b[0;36mextract_frames_from_video\u001b[1;34m(video_path, fps)\u001b[0m\n\u001b[0;32m     14\u001b[0m frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m---> 16\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory_path = './data/Celeb-synthesis/'  # Path to your directory containing videos\n",
    "video_predictions = predict_on_videos_in_directory(directory_path)\n",
    "\n",
    "# Display results for all videos\n",
    "for video_file, prediction in video_predictions:\n",
    "    print(f\"Video: {video_file}, Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cafbfc2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_on_videos_in_directory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/YouTube-real/\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to your directory containing videos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m video_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_on_videos_in_directory\u001b[49m(directory_path)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Display results for all videos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_file, prediction \u001b[38;5;129;01min\u001b[39;00m video_predictions:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_on_videos_in_directory' is not defined"
     ]
    }
   ],
   "source": [
    "directory_path = './data/YouTube-real/'  # Path to your directory containing videos\n",
    "video_predictions = predict_on_videos_in_directory(directory_path)\n",
    "\n",
    "# Display results for all videos\n",
    "for video_file, prediction in video_predictions:\n",
    "    print(f\"Video: {video_file}, Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e650240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005/1005 [==============================] - 17s 16ms/step\n",
      "AUC: 0.78\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Step 4: Get predicted probabilities using the `predict` method\n",
    "y_pred_proba = model.predict(test_dataset)  # This returns probabilities for the positive class\n",
    "\n",
    "# Step 5: Compute ROC curve (FPR, TPR, thresholds)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Step 6: Calculate AUC\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "# Step 7: Plot the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.2f})\", color=\"blue\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Guessing\")\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629c3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to directories\n",
    "\n",
    "rus = RandomOverSampler(random_state=42)\n",
    "all_paths_resampled, all_labels_resampled = rus.fit_resample(all_paths.reshape(-1, 1), all_labels)\n",
    "all_paths_resampled = all_paths_resampled.flatten()\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_paths_resampled, \n",
    "    all_labels_resampled, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=all_labels_resampled\n",
    ")\n",
    "\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = create_dataset(X_train, y_train, shuffle=True)\n",
    "test_dataset = create_dataset(X_test, y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9309632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label distribution: Counter({1: 50910, 0: 50909})\n",
      "Test label distribution: Counter({0: 21819, 1: 21818})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Check the distribution of training labels\n",
    "train_label_counts = Counter(y_train)\n",
    "print(\"Training label distribution:\", train_label_counts)\n",
    "\n",
    "# Check the distribution of test labels\n",
    "test_label_counts = Counter(y_test)\n",
    "print(\"Test label distribution:\", test_label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8efa7d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 1, 1, 1280)       4049571   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,412,068\n",
      "Trainable params: 4,369,277\n",
      "Non-trainable params: 42,791\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the improved model\n",
    "\n",
    "def build_deepfake_model(input_shape=INPUT_SHAPE):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Add fully connected layers with dropout and batch normalization\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=BASE_LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_deepfake_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35382aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1650, compute capability 7.5\n",
      "GPU setup error: Physical devices cannot be modified after being initialized\n",
      "Epoch 1/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 1.0871 - accuracy: 0.6396\n",
      "Epoch 1: val_loss improved from inf to 0.63959, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 402s 108ms/step - loss: 1.0871 - accuracy: 0.6396 - val_loss: 0.6396 - val_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.6923\n",
      "Epoch 2: val_loss did not improve from 0.63959\n",
      "3182/3182 [==============================] - 355s 111ms/step - loss: 0.6192 - accuracy: 0.6923 - val_loss: 0.6701 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.7750\n",
      "Epoch 3: val_loss improved from 0.63959 to 0.43498, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 329s 104ms/step - loss: 0.4991 - accuracy: 0.7750 - val_loss: 0.4350 - val_accuracy: 0.8050 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.8312\n",
      "Epoch 4: val_loss improved from 0.43498 to 0.38791, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 355s 112ms/step - loss: 0.3952 - accuracy: 0.8312 - val_loss: 0.3879 - val_accuracy: 0.8429 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.3878 - accuracy: 0.8321\n",
      "Epoch 5: val_loss improved from 0.38791 to 0.38742, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 317s 100ms/step - loss: 0.3878 - accuracy: 0.8321 - val_loss: 0.3874 - val_accuracy: 0.8454 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8438\n",
      "Epoch 6: val_loss improved from 0.38742 to 0.32886, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 317s 100ms/step - loss: 0.3678 - accuracy: 0.8438 - val_loss: 0.3289 - val_accuracy: 0.8652 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.3366 - accuracy: 0.8579\n",
      "Epoch 7: val_loss improved from 0.32886 to 0.29205, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 318s 100ms/step - loss: 0.3366 - accuracy: 0.8579 - val_loss: 0.2921 - val_accuracy: 0.8837 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.3212 - accuracy: 0.8666\n",
      "Epoch 8: val_loss did not improve from 0.29205\n",
      "3182/3182 [==============================] - 318s 100ms/step - loss: 0.3212 - accuracy: 0.8666 - val_loss: 0.3361 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.3146 - accuracy: 0.8733\n",
      "Epoch 9: val_loss did not improve from 0.29205\n",
      "3182/3182 [==============================] - 317s 100ms/step - loss: 0.3146 - accuracy: 0.8733 - val_loss: 0.3237 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.3107 - accuracy: 0.8751\n",
      "Epoch 10: val_loss did not improve from 0.29205\n",
      "3182/3182 [==============================] - 317s 100ms/step - loss: 0.3107 - accuracy: 0.8751 - val_loss: 0.3291 - val_accuracy: 0.8727 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.2798 - accuracy: 0.8920\n",
      "Epoch 11: val_loss did not improve from 0.29205\n",
      "3182/3182 [==============================] - 319s 100ms/step - loss: 0.2798 - accuracy: 0.8920 - val_loss: 0.3071 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.8803\n",
      "Epoch 12: val_loss did not improve from 0.29205\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3182/3182 [==============================] - 317s 100ms/step - loss: 0.2984 - accuracy: 0.8803 - val_loss: 0.3265 - val_accuracy: 0.8660 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1914 - accuracy: 0.9171\n",
      "Epoch 13: val_loss improved from 0.29205 to 0.18801, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 318s 100ms/step - loss: 0.1914 - accuracy: 0.9171 - val_loss: 0.1880 - val_accuracy: 0.9170 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.9345\n",
      "Epoch 14: val_loss improved from 0.18801 to 0.18373, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 317s 100ms/step - loss: 0.1456 - accuracy: 0.9345 - val_loss: 0.1837 - val_accuracy: 0.9187 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9415\n",
      "Epoch 15: val_loss improved from 0.18373 to 0.17425, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 318s 100ms/step - loss: 0.1285 - accuracy: 0.9415 - val_loss: 0.1743 - val_accuracy: 0.9259 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1183 - accuracy: 0.9463\n",
      "Epoch 16: val_loss improved from 0.17425 to 0.15332, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 319s 100ms/step - loss: 0.1183 - accuracy: 0.9463 - val_loss: 0.1533 - val_accuracy: 0.9328 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9508\n",
      "Epoch 17: val_loss improved from 0.15332 to 0.15133, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 318s 100ms/step - loss: 0.1076 - accuracy: 0.9508 - val_loss: 0.1513 - val_accuracy: 0.9384 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9538\n",
      "Epoch 18: val_loss improved from 0.15133 to 0.14443, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 318s 100ms/step - loss: 0.1011 - accuracy: 0.9538 - val_loss: 0.1444 - val_accuracy: 0.9386 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9560\n",
      "Epoch 19: val_loss did not improve from 0.14443\n",
      "3182/3182 [==============================] - 317s 100ms/step - loss: 0.0961 - accuracy: 0.9560 - val_loss: 0.1448 - val_accuracy: 0.9401 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9584\n",
      "Epoch 20: val_loss improved from 0.14443 to 0.13322, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 319s 100ms/step - loss: 0.0910 - accuracy: 0.9584 - val_loss: 0.1332 - val_accuracy: 0.9404 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9601\n",
      "Epoch 21: val_loss improved from 0.13322 to 0.13038, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 318s 100ms/step - loss: 0.0882 - accuracy: 0.9601 - val_loss: 0.1304 - val_accuracy: 0.9465 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9623\n",
      "Epoch 22: val_loss did not improve from 0.13038\n",
      "3182/3182 [==============================] - 318s 100ms/step - loss: 0.0842 - accuracy: 0.9623 - val_loss: 0.1308 - val_accuracy: 0.9470 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9634\n",
      "Epoch 23: val_loss improved from 0.13038 to 0.12956, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 319s 100ms/step - loss: 0.0817 - accuracy: 0.9634 - val_loss: 0.1296 - val_accuracy: 0.9458 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9653\n",
      "Epoch 24: val_loss did not improve from 0.12956\n",
      "3182/3182 [==============================] - 315s 99ms/step - loss: 0.0784 - accuracy: 0.9653 - val_loss: 0.1301 - val_accuracy: 0.9465 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9665\n",
      "Epoch 25: val_loss did not improve from 0.12956\n",
      "3182/3182 [==============================] - 315s 99ms/step - loss: 0.0754 - accuracy: 0.9665 - val_loss: 0.1347 - val_accuracy: 0.9456 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9669\n",
      "Epoch 26: val_loss did not improve from 0.12956\n",
      "3182/3182 [==============================] - 316s 99ms/step - loss: 0.0747 - accuracy: 0.9669 - val_loss: 0.1443 - val_accuracy: 0.9419 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9686\n",
      "Epoch 27: val_loss did not improve from 0.12956\n",
      "3182/3182 [==============================] - 318s 100ms/step - loss: 0.0710 - accuracy: 0.9686 - val_loss: 0.1750 - val_accuracy: 0.9362 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9692\n",
      "Epoch 28: val_loss did not improve from 0.12956\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3182/3182 [==============================] - 338s 106ms/step - loss: 0.0687 - accuracy: 0.9692 - val_loss: 0.1436 - val_accuracy: 0.9434 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9742\n",
      "Epoch 29: val_loss improved from 0.12956 to 0.12579, saving model to Models\\Eb0_OVS2_best_model_weights.h5\n",
      "3182/3182 [==============================] - 336s 105ms/step - loss: 0.0565 - accuracy: 0.9742 - val_loss: 0.1258 - val_accuracy: 0.9521 - lr: 4.0000e-05\n",
      "Epoch 30/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9758\n",
      "Epoch 30: val_loss did not improve from 0.12579\n",
      "3182/3182 [==============================] - 338s 106ms/step - loss: 0.0516 - accuracy: 0.9758 - val_loss: 0.1443 - val_accuracy: 0.9496 - lr: 4.0000e-05\n",
      "Epoch 31/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9769\n",
      "Epoch 31: val_loss did not improve from 0.12579\n",
      "3182/3182 [==============================] - 341s 107ms/step - loss: 0.0496 - accuracy: 0.9769 - val_loss: 0.1475 - val_accuracy: 0.9499 - lr: 4.0000e-05\n",
      "Epoch 32/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9782\n",
      "Epoch 32: val_loss did not improve from 0.12579\n",
      "3182/3182 [==============================] - 351s 110ms/step - loss: 0.0479 - accuracy: 0.9782 - val_loss: 0.1665 - val_accuracy: 0.9489 - lr: 4.0000e-05\n",
      "Epoch 33/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9783\n",
      "Epoch 33: val_loss did not improve from 0.12579\n",
      "3182/3182 [==============================] - 467s 147ms/step - loss: 0.0475 - accuracy: 0.9783 - val_loss: 0.1477 - val_accuracy: 0.9501 - lr: 4.0000e-05\n",
      "Epoch 34/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9786\n",
      "Epoch 34: val_loss did not improve from 0.12579\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "3182/3182 [==============================] - 297s 93ms/step - loss: 0.0465 - accuracy: 0.9786 - val_loss: 0.1370 - val_accuracy: 0.9518 - lr: 4.0000e-05\n",
      "Epoch 35/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9806\n",
      "Epoch 35: val_loss did not improve from 0.12579\n",
      "3182/3182 [==============================] - 302s 95ms/step - loss: 0.0431 - accuracy: 0.9806 - val_loss: 0.1420 - val_accuracy: 0.9533 - lr: 8.0000e-06\n",
      "Epoch 36/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9808\n",
      "Epoch 36: val_loss did not improve from 0.12579\n",
      "3182/3182 [==============================] - 295s 93ms/step - loss: 0.0421 - accuracy: 0.9808 - val_loss: 0.1524 - val_accuracy: 0.9502 - lr: 8.0000e-06\n",
      "Epoch 37/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9814\n",
      "Epoch 37: val_loss did not improve from 0.12579\n",
      "3182/3182 [==============================] - 294s 92ms/step - loss: 0.0417 - accuracy: 0.9814 - val_loss: 0.1497 - val_accuracy: 0.9520 - lr: 8.0000e-06\n",
      "Epoch 38/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9813\n",
      "Epoch 38: val_loss did not improve from 0.12579\n",
      "3182/3182 [==============================] - 294s 92ms/step - loss: 0.0417 - accuracy: 0.9813 - val_loss: 0.1481 - val_accuracy: 0.9510 - lr: 8.0000e-06\n",
      "Epoch 39/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9814\n",
      "Epoch 39: val_loss did not improve from 0.12579\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "3182/3182 [==============================] - 295s 93ms/step - loss: 0.0408 - accuracy: 0.9814 - val_loss: 0.1491 - val_accuracy: 0.9519 - lr: 8.0000e-06\n",
      "Epoch 39: early stopping\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 51\u001b[0m\n\u001b[0;32m     42\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     43\u001b[0m     train_dataset,\n\u001b[0;32m     44\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m1.0\u001b[39m}  \u001b[38;5;66;03m# Balance weights if needed\u001b[39;00m\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Save the entire model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mModels/Eb0_OVS2_best_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "\n",
    "# Enable mixed precision training\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU setup error: {e}\")\n",
    "    \n",
    "    \n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='Models/Eb0_OVS2_best_model_weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Cosine decay learning rate scheduler\n",
    "cosine_decay = CosineDecay(initial_learning_rate=BASE_LEARNING_RATE, decay_steps=EPOCHS, alpha=0.1)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback],\n",
    "    class_weight={0: 1.0, 1: 1.0}  # Balance weights if needed\n",
    ")\n",
    "\n",
    "# Save the entire model\n",
    "model.save('Models/Eb0_OVS2_best_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c2bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (64, 64)\n",
    "INPUT_SHAPE = (64, 64, 3)\n",
    "EPOCHS = 100\n",
    "BASE_LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7cadf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)  # Resize to match input size of the model\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image, label\n",
    "\n",
    "def create_dataset(image_paths, labels, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(lambda x, y: load_image(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE).cache()\n",
    "    return dataset\n",
    "\n",
    "def extract_paths_and_labels(directory_path, label):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for subdir, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                image_paths.append(os.path.join(subdir, file))\n",
    "                labels.append(label)\n",
    "    return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad00fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dir = './data/processed/real/'\n",
    "fake_dir = './data/processed/fake/'\n",
    "\n",
    "# Extract paths and labels\n",
    "real_paths, real_labels = extract_paths_and_labels(real_dir, label=0)\n",
    "fake_paths, fake_labels = extract_paths_and_labels(fake_dir, label=1)\n",
    "\n",
    "# Combine paths and labels\n",
    "all_paths = np.array(real_paths + fake_paths)\n",
    "all_labels = np.array(real_labels + fake_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e9b4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to directories\n",
    "\n",
    "rus = RandomOverSampler(random_state=42)\n",
    "all_paths_resampled, all_labels_resampled = rus.fit_resample(all_paths.reshape(-1, 1), all_labels)\n",
    "all_paths_resampled = all_paths_resampled.flatten()\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_paths_resampled, \n",
    "    all_labels_resampled, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=all_labels_resampled\n",
    ")\n",
    "\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = create_dataset(X_train, y_train, shuffle=True)\n",
    "test_dataset = create_dataset(X_test, y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e4bf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 2, 2, 1280)       4049571   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,412,068\n",
      "Trainable params: 4,369,277\n",
      "Non-trainable params: 42,791\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the improved model\n",
    "\n",
    "def build_deepfake_model(input_shape=INPUT_SHAPE):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=True)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Add fully connected layers with dropout and batch normalization\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=BASE_LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_deepfake_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca70a36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1650, compute capability 7.5\n",
      "GPU setup error: Physical devices cannot be modified after being initialized\n",
      "Epoch 1/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.8177 - accuracy: 0.8255\n",
      "Epoch 1: val_loss improved from inf to 0.24497, saving model to Models\\Eb0_OVS3_best_model_weights.h5\n",
      "3182/3182 [==============================] - 927s 184ms/step - loss: 0.8177 - accuracy: 0.8255 - val_loss: 0.2450 - val_accuracy: 0.9040 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.9102\n",
      "Epoch 2: val_loss did not improve from 0.24497\n",
      "3182/3182 [==============================] - 433s 136ms/step - loss: 0.2373 - accuracy: 0.9102 - val_loss: 0.3175 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.2078 - accuracy: 0.9273\n",
      "Epoch 3: val_loss improved from 0.24497 to 0.22699, saving model to Models\\Eb0_OVS3_best_model_weights.h5\n",
      "3182/3182 [==============================] - 448s 141ms/step - loss: 0.2078 - accuracy: 0.9273 - val_loss: 0.2270 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1960 - accuracy: 0.9288\n",
      "Epoch 4: val_loss did not improve from 0.22699\n",
      "3182/3182 [==============================] - 436s 137ms/step - loss: 0.1960 - accuracy: 0.9288 - val_loss: 0.5595 - val_accuracy: 0.7968 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1742 - accuracy: 0.9366\n",
      "Epoch 5: val_loss did not improve from 0.22699\n",
      "3182/3182 [==============================] - 427s 134ms/step - loss: 0.1742 - accuracy: 0.9366 - val_loss: 0.4923 - val_accuracy: 0.8327 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1832 - accuracy: 0.9321\n",
      "Epoch 6: val_loss did not improve from 0.22699\n",
      "3182/3182 [==============================] - 422s 133ms/step - loss: 0.1832 - accuracy: 0.9321 - val_loss: 0.3926 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1640 - accuracy: 0.9413\n",
      "Epoch 7: val_loss improved from 0.22699 to 0.16135, saving model to Models\\Eb0_OVS3_best_model_weights.h5\n",
      "3182/3182 [==============================] - 436s 137ms/step - loss: 0.1640 - accuracy: 0.9413 - val_loss: 0.1614 - val_accuracy: 0.9417 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1333 - accuracy: 0.9501\n",
      "Epoch 8: val_loss did not improve from 0.16135\n",
      "3182/3182 [==============================] - 428s 135ms/step - loss: 0.1333 - accuracy: 0.9501 - val_loss: 0.7829 - val_accuracy: 0.7661 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9506\n",
      "Epoch 9: val_loss did not improve from 0.16135\n",
      "3182/3182 [==============================] - 447s 140ms/step - loss: 0.1244 - accuracy: 0.9506 - val_loss: 0.6353 - val_accuracy: 0.8496 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1159 - accuracy: 0.9550\n",
      "Epoch 10: val_loss did not improve from 0.16135\n",
      "3182/3182 [==============================] - 436s 137ms/step - loss: 0.1159 - accuracy: 0.9550 - val_loss: 0.6744 - val_accuracy: 0.8629 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9562\n",
      "Epoch 11: val_loss did not improve from 0.16135\n",
      "3182/3182 [==============================] - 440s 138ms/step - loss: 0.1121 - accuracy: 0.9562 - val_loss: 0.2004 - val_accuracy: 0.9412 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9598\n",
      "Epoch 12: val_loss did not improve from 0.16135\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "3182/3182 [==============================] - 436s 137ms/step - loss: 0.1016 - accuracy: 0.9598 - val_loss: 0.2202 - val_accuracy: 0.9301 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9685\n",
      "Epoch 13: val_loss improved from 0.16135 to 0.09514, saving model to Models\\Eb0_OVS3_best_model_weights.h5\n",
      "3182/3182 [==============================] - 447s 141ms/step - loss: 0.0639 - accuracy: 0.9685 - val_loss: 0.0951 - val_accuracy: 0.9596 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9712\n",
      "Epoch 14: val_loss did not improve from 0.09514\n",
      "3182/3182 [==============================] - 441s 139ms/step - loss: 0.0587 - accuracy: 0.9712 - val_loss: 0.0965 - val_accuracy: 0.9616 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9726\n",
      "Epoch 15: val_loss did not improve from 0.09514\n",
      "3182/3182 [==============================] - 439s 138ms/step - loss: 0.0555 - accuracy: 0.9726 - val_loss: 0.1677 - val_accuracy: 0.9579 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9744\n",
      "Epoch 16: val_loss did not improve from 0.09514\n",
      "3182/3182 [==============================] - 438s 138ms/step - loss: 0.0531 - accuracy: 0.9744 - val_loss: 0.1050 - val_accuracy: 0.9621 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9761\n",
      "Epoch 17: val_loss did not improve from 0.09514\n",
      "3182/3182 [==============================] - 444s 140ms/step - loss: 0.0512 - accuracy: 0.9761 - val_loss: 0.2859 - val_accuracy: 0.9534 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9766\n",
      "Epoch 18: val_loss did not improve from 0.09514\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "3182/3182 [==============================] - 433s 136ms/step - loss: 0.0487 - accuracy: 0.9766 - val_loss: 0.1171 - val_accuracy: 0.9622 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9789\n",
      "Epoch 19: val_loss did not improve from 0.09514\n",
      "3182/3182 [==============================] - 435s 137ms/step - loss: 0.0419 - accuracy: 0.9789 - val_loss: 0.0986 - val_accuracy: 0.9649 - lr: 4.0000e-05\n",
      "Epoch 20/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9806\n",
      "Epoch 20: val_loss did not improve from 0.09514\n",
      "3182/3182 [==============================] - 438s 138ms/step - loss: 0.0391 - accuracy: 0.9806 - val_loss: 0.1204 - val_accuracy: 0.9651 - lr: 4.0000e-05\n",
      "Epoch 21/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9821\n",
      "Epoch 21: val_loss did not improve from 0.09514\n",
      "3182/3182 [==============================] - 434s 137ms/step - loss: 0.0372 - accuracy: 0.9821 - val_loss: 0.1063 - val_accuracy: 0.9648 - lr: 4.0000e-05\n",
      "Epoch 22/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9830\n",
      "Epoch 22: val_loss did not improve from 0.09514\n",
      "3182/3182 [==============================] - 439s 138ms/step - loss: 0.0361 - accuracy: 0.9830 - val_loss: 0.1593 - val_accuracy: 0.9639 - lr: 4.0000e-05\n",
      "Epoch 23/100\n",
      "3182/3182 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9836\n",
      "Epoch 23: val_loss did not improve from 0.09514\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "3182/3182 [==============================] - 438s 138ms/step - loss: 0.0348 - accuracy: 0.9836 - val_loss: 0.1087 - val_accuracy: 0.9652 - lr: 4.0000e-05\n",
      "Epoch 23: early stopping\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 51\u001b[0m\n\u001b[0;32m     42\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     43\u001b[0m     train_dataset,\n\u001b[0;32m     44\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m1.0\u001b[39m}  \u001b[38;5;66;03m# Balance weights if needed\u001b[39;00m\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Save the entire model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mModels/Eb0_OVS3_best_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IIITR\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "\n",
    "# Enable mixed precision training\n",
    "set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU setup error: {e}\")\n",
    "    \n",
    "    \n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='Models/Eb0_OVS3_best_model_weights.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Cosine decay learning rate scheduler\n",
    "cosine_decay = CosineDecay(initial_learning_rate=BASE_LEARNING_RATE, decay_steps=EPOCHS, alpha=0.1)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback],\n",
    "    class_weight={0: 1.0, 1: 1.0}  # Balance weights if needed\n",
    ")\n",
    "\n",
    "# Save the entire model\n",
    "model.save('Models/Eb0_OVS3_best_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743163b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SA",
   "language": "python",
   "name": "iiitr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
